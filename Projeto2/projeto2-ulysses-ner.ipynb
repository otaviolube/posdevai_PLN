{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural\n",
    "\n",
    "**Prof. Dr. Hilário Thomaz Alves de Oliveira**  \n",
    "**Pós-graduação em Desenvolvimento de Aplicações Inteligentes**  \n",
    "**Processamento de Linguagem Natural — Projeto 01 - Classificação de Decisões Judiciais**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nome:** Otávio Lube dos Santos  \n",
    "**Matrícula:** 20231DEVAI0157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (24.3.1)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from seqeval) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16217 sha256=4408294ecdb8a23320404dee0ef3db163d183d8e0d670e7163dfc0539d12a6ed\n",
      "  Stored in directory: /Users/otaviolube/Library/Caches/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Collecting sklearn_crfsuite\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn_crfsuite)\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from sklearn_crfsuite) (1.6.1)\n",
      "Collecting tabulate>=0.4.2 (from sklearn_crfsuite)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from sklearn_crfsuite) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.5.0)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp311-cp311-macosx_11_0_arm64.whl (319 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, python-crfsuite, sklearn_crfsuite\n",
      "Successfully installed python-crfsuite-0.9.11 sklearn_crfsuite-0.5.0 tabulate-0.9.0\n",
      "Collecting scikit-learn==1.3.2\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting numpy<2.0,>=1.17.3 (from scikit-learn==1.3.2)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn==1.3.2) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn==1.3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/otaviolube/Desktop/pos-devai-ifes/6-proc-lig-nat/.venv/lib/python3.11/site-packages (from scikit-learn==1.3.2) (3.5.0)\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scikit-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed numpy-1.26.4 scikit-learn-1.3.2\n",
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install seqeval\n",
    "!pip install -U sklearn_crfsuite\n",
    "!pip install scikit-learn==1.3.2\n",
    "\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-13 17:31:41--  https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/train.conll\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2142199 (2.0M) [text/plain]\n",
      "Saving to: ‘train.conll’\n",
      "\n",
      "train.conll         100%[===================>]   2.04M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-01-13 17:31:42 (13.7 MB/s) - ‘train.conll’ saved [2142199/2142199]\n",
      "\n",
      "--2025-01-13 17:31:42--  https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/test.conll\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 438441 (428K) [text/plain]\n",
      "Saving to: ‘test.conll’\n",
      "\n",
      "test.conll          100%[===================>] 428.17K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-01-13 17:31:43 (511 MB/s) - ‘test.conll’ saved [438441/438441]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/train.conll\n",
    "!wget https://raw.githubusercontent.com/messias077/ner_pt/main/data/corpora/le_ner/test.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/main/annotated-corpora/PL_corpus_conll/pl_corpus_categorias/train.txt\n",
    "# !wget https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/main/annotated-corpora/PL_corpus_conll/pl_corpus_categorias/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os repositórios acima estão indisponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-13 17:39:32--  https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/main/C-corpus_v2/pl_corpus_categorias/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 779304 (761K) [text/plain]\n",
      "Saving to: ‘train.txt.1’\n",
      "\n",
      "train.txt.1         100%[===================>] 761.04K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-01-13 17:39:33 (468 MB/s) - ‘train.txt.1’ saved [779304/779304]\n",
      "\n",
      "--2025-01-13 17:39:33--  https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/main/C-corpus_v2/pl_corpus_categorias/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 185614 (181K) [text/plain]\n",
      "Saving to: ‘test.txt’\n",
      "\n",
      "test.txt            100%[===================>] 181.26K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-01-13 17:39:34 (452 MB/s) - ‘test.txt’ saved [185614/185614]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/main/C-corpus_v2/pl_corpus_categorias/train.txt\n",
    "!wget https://raw.githubusercontent.com/ulysses-camara/ulysses-ner-br/main/C-corpus_v2/pl_corpus_categorias/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_file(corpus_file, delimiter='\\t', ner_column=1):\n",
    "    with open(corpus_file, encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    data = []\n",
    "    words = []\n",
    "    tags = []\n",
    "    for line in lines:\n",
    "        line = line.replace('\\n', '')\n",
    "        if line != '':\n",
    "            if delimiter in line:\n",
    "                fragments = line.split(delimiter)\n",
    "                words.append(fragments[0])\n",
    "                tags.append(fragments[ner_column])\n",
    "        else:\n",
    "            if len(words) > 1:\n",
    "                data.append((words, tags))\n",
    "            words = []\n",
    "            tags = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus: ulysses_ner\n",
      "\n",
      "Train data: 2269\n",
      "Test data: 523\n"
     ]
    }
   ],
   "source": [
    "corpus_name = 'ulysses_ner'\n",
    "\n",
    "report_dir = 'report/'\n",
    "\n",
    "train_file = None\n",
    "test_file = None\n",
    "\n",
    "id_ner = 1\n",
    "delimiter = '\\t'\n",
    "\n",
    "if corpus_name == 'le_ner':\n",
    "  train_file = './train.conll'\n",
    "  test_file = './test.conll'\n",
    "  delimiter = ' '\n",
    "elif corpus_name == 'ulysses_ner':\n",
    "  train_file = './train.txt'\n",
    "  test_file = './test.txt'\n",
    "  delimiter = ' '\n",
    "\n",
    "print(f'\\nCorpus: {corpus_name}')\n",
    "\n",
    "report_dir = os.path.join(report_dir, corpus_name)\n",
    "\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "train_data = read_corpus_file(train_file, delimiter=delimiter, ner_column=id_ner)\n",
    "test_data = read_corpus_file(test_file, delimiter=delimiter, ner_column=id_ner)\n",
    "\n",
    "print(f'\\nTrain data: {len(train_data)}')\n",
    "print(f'Test data: {len(test_data)}')\n",
    "\n",
    "test_data_original = np.array(test_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sala', 'das', 'Sessões', ',', 'em', 'de', 'de', '2019', '.'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATA', 'O'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    nlp = spacy.load(name='pt_core_news_sm',\n",
    "                     disable=['parser', 'ner', 'lemmatizer', 'textcat'])\n",
    "    preprocessed_data = []\n",
    "    for d in data:\n",
    "        sentence = ' '.join(d[0])\n",
    "        doc = nlp(sentence)\n",
    "        pos_tags = [t.pos_ for t in doc]\n",
    "        preprocessed_data.append((d[0], pos_tags, d[1]))\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_preprocessing(train_data)\n",
    "\n",
    "test_data = data_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sala', 'das', 'Sessões', ',', 'em', 'de', 'de', '2019', '.'],\n",
       " ['PROPN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'ADP', 'ADP', 'NUM', 'PUNCT'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATA', 'O'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sent_features(sentence):\n",
    "    return [extract_features(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\n",
    "def extract_labels(sentence):\n",
    "    return [label for _, _, label in sentence]\n",
    "\n",
    "\n",
    "def extract_features(sentence, i):\n",
    "    word = sentence[i][0]\n",
    "    postag = sentence[i][1]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "        'word.islower()': word.islower(),\n",
    "        'word[0].isupper()': word[0].isupper(),\n",
    "        'word[0].islower()': word[0].islower(),\n",
    "        'not word[0].isalnum()': not word[0].isalnum(),\n",
    "        'not word.isalnum()': not word.isalnum(),\n",
    "        'word.isalpha()': word.isalpha()\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sentence[i - 1][0]\n",
    "        postag1 = sentence[i - 1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "            '-1:word.islower()': word1.islower()\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True # BOS = Begin of Sentence\n",
    "    if i > 1:\n",
    "        word1 = sentence[i - 2][0]\n",
    "        postag1 = sentence[i - 2][1]\n",
    "        features.update({\n",
    "            '-2:word.lower()': word1.lower(),\n",
    "            '-2:word.istitle()': word1.istitle(),\n",
    "            '-2:word.isupper()': word1.isupper(),\n",
    "            '-2:postag': postag1,\n",
    "            '-2:postag[:2]': postag1[:2],\n",
    "            '-2:word.islower()': word1.islower()\n",
    "        })\n",
    "    if i < len(sentence) - 1:\n",
    "        word1 = sentence[i + 1][0]\n",
    "        postag1 = sentence[i + 1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "            '+1:word.islower()': word1.islower()\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # EOS = End of Sentence\n",
    "    if i < len(sentence) - 2:\n",
    "        word1 = sentence[i + 2][0]\n",
    "        postag1 = sentence[i + 2][1]\n",
    "        features.update({\n",
    "            '+2:word.lower()': word1.lower(),\n",
    "            '+2:word.istitle()': word1.istitle(),\n",
    "            '+2:word.isupper()': word1.isupper(),\n",
    "            '+2:postag': postag1,\n",
    "            '+2:postag[:2]': postag1[:2],\n",
    "            '+2:word.islower()': word1.islower()\n",
    "        })\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_data(data):\n",
    "    sentences = []\n",
    "    for d in data:\n",
    "        sentences.append(list(zip(d[0], d[1], d[2])))\n",
    "    x_data = [extract_sent_features(s) for s in sentences]\n",
    "    y_data = [extract_labels(s) for s in sentences]\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = convert_data(train_data)\n",
    "\n",
    "X_test, y_test = convert_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example features: [{'bias': 1.0, 'word.lower()': 'sala', 'word[-3:]': 'ala', 'word[-2:]': 'la', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, 'BOS': True, '+1:word.lower()': 'das', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': True, '+2:word.lower()': 'sessões', '+2:word.istitle()': True, '+2:word.isupper()': False, '+2:postag': 'PROPN', '+2:postag[:2]': 'PR', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'das', 'word[-3:]': 'das', 'word[-2:]': 'as', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': True, 'word[0].isupper()': False, 'word[0].islower()': True, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'sala', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '+1:word.lower()': 'sessões', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'PROPN', '+1:postag[:2]': 'PR', '+1:word.islower()': False, '+2:word.lower()': ',', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'sessões', 'word[-3:]': 'ões', 'word[-2:]': 'es', 'word.isupper()': False, 'word.istitle()': True, 'word.isdigit()': False, 'postag': 'PROPN', 'postag[:2]': 'PR', 'word.islower()': False, 'word[0].isupper()': True, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'das', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': True, '-2:word.lower()': 'sala', '-2:word.istitle()': True, '-2:word.isupper()': False, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': ',', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False, '+2:word.lower()': 'em', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': True}, {'bias': 1.0, 'word.lower()': ',', 'word[-3:]': ',', 'word[-2:]': ',', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': 'sessões', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'PROPN', '-1:postag[:2]': 'PR', '-1:word.islower()': False, '-2:word.lower()': 'das', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': True, '+1:word.lower()': 'em', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': True, '+2:word.lower()': 'de', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': True}, {'bias': 1.0, 'word.lower()': 'em', 'word[-3:]': 'em', 'word[-2:]': 'em', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': True, 'word[0].isupper()': False, 'word[0].islower()': True, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': ',', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PUNCT', '-1:postag[:2]': 'PU', '-1:word.islower()': False, '-2:word.lower()': 'sessões', '-2:word.istitle()': True, '-2:word.isupper()': False, '-2:postag': 'PROPN', '-2:postag[:2]': 'PR', '-2:word.islower()': False, '+1:word.lower()': 'de', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': True, '+2:word.lower()': 'de', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'ADP', '+2:postag[:2]': 'AD', '+2:word.islower()': True}, {'bias': 1.0, 'word.lower()': 'de', 'word[-3:]': 'de', 'word[-2:]': 'de', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': True, 'word[0].isupper()': False, 'word[0].islower()': True, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'em', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': True, '-2:word.lower()': ',', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'PUNCT', '-2:postag[:2]': 'PU', '-2:word.islower()': False, '+1:word.lower()': 'de', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'ADP', '+1:postag[:2]': 'AD', '+1:word.islower()': True, '+2:word.lower()': '2019', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'NUM', '+2:postag[:2]': 'NU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': 'de', 'word[-3:]': 'de', 'word[-2:]': 'de', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'ADP', 'postag[:2]': 'AD', 'word.islower()': True, 'word[0].isupper()': False, 'word[0].islower()': True, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': True, '-1:word.lower()': 'de', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': True, '-2:word.lower()': 'em', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': True, '+1:word.lower()': '2019', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NUM', '+1:postag[:2]': 'NU', '+1:word.islower()': False, '+2:word.lower()': '.', '+2:word.istitle()': False, '+2:word.isupper()': False, '+2:postag': 'PUNCT', '+2:postag[:2]': 'PU', '+2:word.islower()': False}, {'bias': 1.0, 'word.lower()': '2019', 'word[-3:]': '019', 'word[-2:]': '19', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': True, 'postag': 'NUM', 'postag[:2]': 'NU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': False, 'not word.isalnum()': False, 'word.isalpha()': False, '-1:word.lower()': 'de', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'ADP', '-1:postag[:2]': 'AD', '-1:word.islower()': True, '-2:word.lower()': 'de', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': True, '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PUNCT', '+1:postag[:2]': 'PU', '+1:word.islower()': False}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'word.isdigit()': False, 'postag': 'PUNCT', 'postag[:2]': 'PU', 'word.islower()': False, 'word[0].isupper()': False, 'word[0].islower()': False, 'not word[0].isalnum()': True, 'not word.isalnum()': True, 'word.isalpha()': False, '-1:word.lower()': '2019', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NUM', '-1:postag[:2]': 'NU', '-1:word.islower()': False, '-2:word.lower()': 'de', '-2:word.istitle()': False, '-2:word.isupper()': False, '-2:postag': 'ADP', '-2:postag[:2]': 'AD', '-2:word.islower()': True, 'EOS': True}]\n",
      "\n",
      "Label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATA', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nExample features: {X_train[0]}')\n",
    "\n",
    "print(f'\\nLabel: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = CRF(max_iterations=50, c1=0.1, c2=0.1, all_possible_transitions=False)\n",
    "\n",
    "\n",
    "# Testar com max_iterations=100 e all_possible_transitions=True\n",
    "crf = CRF(max_iterations=100, c1=0.1, c2=0.1, all_possible_transitions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DATA       0.96      0.92      0.94        98\n",
      "      EVENTO       1.00      0.22      0.36         9\n",
      "  FUNDAMENTO       0.82      0.83      0.82       124\n",
      "       LOCAL       0.75      0.72      0.74       101\n",
      " ORGANIZACAO       0.76      0.69      0.72        94\n",
      "      PESSOA       0.90      0.78      0.84       119\n",
      "PRODUTODELEI       0.69      0.67      0.68        54\n",
      "\n",
      "   micro avg       0.82      0.77      0.80       599\n",
      "   macro avg       0.84      0.69      0.73       599\n",
      "weighted avg       0.83      0.77      0.79       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "report_df = DataFrame(classification_report(y_test, y_pred, output_dict=True))\n",
    "report_df.T.to_excel(os.path.join(report_dir, f'tarefa2-{corpus_name}.xlsx'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
